% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Evaluating Machine Translation models using both \\
	 Direct Assessment and automatic metrics \\
		\hfill \\
		\hfill \\
		\small{1st Semester of 2021-2022}}

  \author{Mihnea Mihai \\
  \texttt{mihnea.mihai7@s.unibuc.ro} }

\begin{document}
\maketitle
\begin{abstract}
This report presents the evaluation of three pretrained machine translation models
(\textbf{LibreTranslate}, \textbf{OPUS} and \textbf{MBart50})
for the language pairs \textbf{German} to \textbf{English} and \textbf{English} to \textbf{Romanian}
with a test suite comprising several domains
(\textbf{legal}, \textbf{medical}, \textbf{news} and \textbf{literary})
based both on self-provided Direct Assessment adequacy scores and
automatic scores (\textbf{BLEU}).
\end{abstract}

\section{Introduction}
\label{section:intro}

\subsection{Models}

This report compares several pretrained models in order to provide useful insights
pertaining to their relative performance in various domains
and help compare and align the hierarchy generated by direct assessment
with the one generated by automatic evaluation methods.

\subsubsection{LibreTranslate}

LibreTranslate\footnote{\url{https://libretranslate.com/}}
is an open-source Machine Translation model providing a free public API.
It does not rely on proprietary providers,
instead it is powered by the open source
\textbf{Argos Translate}\footnote{\url{https://www.argosopentech.com/}} library,
which in turn uses \textbf{OpenNMT}\footnote{\url{https://opennmt.net/}}
\citep{klein-etal-2017-opennmt} for translation.

\subsubsection{OPUS}
OPUS\footnote{\url{https://github.com/Helsinki-NLP/OPUS-MT}}
\citep{tiedemann-thottingal-2020-opus} is an open translation model based on
\textbf{Marian-NMT}\footnote{\url{https://marian-nmt.github.io/}} \citep{mariannmt}
framework and trained on the \textbf{OPUS}\footnote{\url{https://opus.nlpl.eu/}} corpus.

\subsubsection{MBart-50}
MBart-50\footnote{\url{https://github.com/pytorch/fairseq/tree/main/examples/mbart}} \citep{liu2020multilingual} is a denoising autoencoder pretrained on large monolingual corpora
using the BART \citep{lewis-etal-2020-bart} objective.

\subsection{Test suites}

The evaluation is done on several test sets from different domains, in order to provide
an accurate depiction of the models' performance.
All sentences have been hand-picked in order to ensure variate distribution of
long, short, context-dependant or context-independent sentences.

\subsubsection{News}

The news test set (\textbf{German}>\textbf{English})
is extracted from the news translation
task\footnote{\url{https://github.com/wmt-conference/wmt21-news-systems/}} of the
Sixth Conference on Machine Translation (WMT21).
It contains news article excerpts originally in German with an
English translation as reference.

\subsubsection{Legal}

The legal test set (\textbf{German}>\textbf{English})
is manually built based on German law and its translation into English
provided by the German Federal Ministry of Justice\footnote{
\url{https://www.gesetze-im-internet.de/Teilliste_translations.html}},
which makes it valuable as it comprises actual law jargon in current use in Germany
and translations provided by law experts.

\subsubsection{Medical}

The medical test set (\textbf{English}>\textbf{Romanian}) is extracted from a
\href{https://elrc-share.eu/repository/browse/multilingual-corpus-made-out-of-pdf-documents-from-the-european-medicines-agency-emea-httpswwwemaeuropaeu-february-2020/3cf9da8e858511ea913100155d0267062d01c2d847c349628584d10293948de3/}{multilingual corpus}
made of documents from the European Medicine Agency (EMA).

\subsubsection{Literary}

The literary test set (\textbf{English}>\textbf{Romanian}) is extracted
from the MULTEX-East "1984" \citep{11356/1043} annotated corpus
of the novel \textbf{1984} by George Orwell
comprising the original in English
and an official Romanian translation.

\section{Approach}
\label{section:approach}

The code and processed data is available \textbf{\href{https://github.com/mihnea-mihai/mt-eval}{here}}.

The test sentences were extracted from the sources mentioned above
and were processed into JSON format for ease of use and access.

In order to obtain the translations to be evaluated we used either
the model's own API endpoint (in the case of LibreTranslate)
or loaded the pretrained model from the
\textbf{Huggingface}\footnote{\url{https://huggingface.co/}} framework.

The direct assessment scores for adequacy were provided manually
in a simplified CLI interface.

In order to ensure reproducibility, the automation scores were computed
using the Python library \texttt{\href{https://github.com/mjpost/sacrebleu}{sacrebleu}}
\citep{post-2018-call}.

\subsection{Automatic metrics}

As the manual assessment through annotators requires significant resources,
automatic metrics are a possible alternative for 

\subsubsection{BLEU score}

The BLEU score \citep{papineni-etal-2002-bleu}
is the standard metric of automatically assessing machine
translation output quality.

It counts the number of n-grams of the output also occurring in the reference translation(s)
while dividing by the total number of n-grams in the reference(s).

The precision scores are computed for several values of n (usually 4)
and their logarithms are averaged with uniform weights, which equivalent to the geometric mean.

Additional adjustments are required in order to prevent repeated n-grams
to be counted more than their actual number of occurrences in the reference(s).

Lastly, brevity penalties are also applied
for the case where most n-grams in the output do match the reference, but
some n-grams from the reference are missing completely.

\subsubsection{chrF score}

The character n-gram F-score \citep{popovic-2015-chrf} extends the BLEU score
by taking into consideration not only precision (n-grams in the output with counterparts in the reference),
but also recall (n-grams in the reference with counterparts in the output).

\subsubsection{TER score}
The Translation Edit Rate \citep{snover-etal-2006-study} is another automatic metric
computing the edits needed to make the output identical to the reference,
relative to the whole length of the reference.

\section{Conclusions and Future Work}
\label{section:conclusions}

\subsection{Correlation}

Although automatic evaluation may not be ideal, we can observe
a strong correlation between the direct assessment results and the
metrics obtained automatically, which again proves their extreme usefulness.

\subsection{Risks}

In the MT evaluation task the risk of overfitting must be
seriously taken into account, as some corpora are not split in test sets and train test
and there is the possibility that a pretrained model already had knowledge
of some test sentences.

\subsection{Hierarchy}

On average, the opensource \textbf{LibreTranslate} model
performed below the other two models.

In some domains, \textbf{OPUS} clearly surpassed \textbf{MBart-50} in performance,
whereas in others they are very similar (given also the relatively limited sample size).

\subsection{Architecture difference}

The fact that \textbf{MBart-50} has a noising step in its processing pipeline can be noticed
by the several 100\% fluent outputs which had however missing, conflicting or even random words.

\begin{table}
    \centering
    \begin{tabular}{lcccc}
    \hline
        & BLEU & chrF & TER & DA \\
    \hline
        OPUS & \textbf{46.67} & \textbf{71.09} & \textbf{40.24} & \textbf{92.6}\\
        MBart-50 & 25.84 & 57.00 & 62.02 & 70.5
    \end{tabular}
    \caption{Medical domain (EN>RO)}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lcccc}
    \hline
        & BLEU & chrF & TER & DA\\
    \hline
        OPUS & \textbf{19.64} & \textbf{43.68} & \textbf{77.39} & \textbf{71.52}\\
        MBart-50 & 15.45 & 40.74 & 82.04 & 64.4
    \end{tabular}
    \caption{Literary domain (EN>RO)}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lcccc}
    \hline
        & BLEU & chrF & TER & DA\\
    \hline
        LibreTranslate & 22.85 & 52.25 & 58.99 & 64.61 \\
        OPUS & \textbf{32.80} & \textbf{61.34} & \textbf{52.27} & 84.61 \\
        MBart-50 & 28.05 & 58.95 & 54.19 & \textbf{86.34}
    \end{tabular}
    \caption{News domain (DE>EN)}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lcccc}
    \hline
        & BLEU & chrF & TER & DA\\
    \hline
        LibreTranslate & 15.35 & 41.49 & 63.28 & 61.6\\
        OPUS & 21.24 & \textbf{47.84} & 59.56 & \textbf{85.4} \\
        MBart-50 & \textbf{24.38} & 51.37 & \textbf{58.03} & 83.5
    \end{tabular}
    \caption{Legal domain (DE>EN)}
\end{table}


% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}


\end{document}
